
num_gpus=4
for seed in $(seq 1 5); do
OUTPUT_DIR=sst2_rank_16_s${seed}
NCCL_P2P_DISABLE=1 nohup python -m torch.distributed.launch --nproc_per_node=$num_gpus --master_port=12345 non-GPT-2/examples/pytorch/text-classification/run_glue.py      --save_total_limit 1      --model_name_or_path bert-base-uncased      --task_name sst2      --output_dir ${OUTPUT_DIR}      --do_train      --do_eval      --num_train_epochs 5      --save_steps 1000      --seed ${seed}      --per_device_train_batch_size 8      --max_seq_length 128      --per_device_eval_batch_size 8      --overwrite_output_dir      --logging_steps 1000      --load_best_model_at_end True      --metric_for_best_model eval_accuracy      --apply_lora        --lora_r 16  --apply_sparse --num_sparse 16    --evaluation_strategy steps --learning_rate 2e-4 > sst2_rank_16_sparse_16_${seed}.out

NCCL_P2P_DISABLE=1 python -m torch.distributed.launch --nproc_per_node=4 --master_port=12342 non-GPT-2/examples/pytorch/text-classification/run_glue_prune_with_WAB.py      --save_total_limit 1      --model_name_or_path sst2_rank_16_s${seed}     --task_name sst2      --output_dir sst2_rank_16_s1_ABS      --do_train      --do_eval      --num_train_epochs 5      --save_steps 100      --seed ${seed}     --per_device_train_batch_size 8      --max_seq_length 128      --per_device_eval_batch_size 8      --overwrite_output_dir      --logging_steps 100      --load_best_model_at_end True      --metric_for_best_model eval_accuracy      --apply_lora      --apply_sparse      --num_sparse 16          --lora_r 16      --evaluation_strategy steps --learning_rate 2e-4 --pruning_ratio 0.2 --ad_pruning_ratio 0.2 --prune_mode ABS > sst2_rank_16_ABS_${seed}_0.2.out 